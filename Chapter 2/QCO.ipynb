{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02081049-979c-44c0-84c6-41f4e3c2735d",
   "metadata": {},
   "source": [
    "# Quantum Circuit Optimization with Reinforcement Learning on qBraid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94688482-3ccc-4857-9134-cc29b80e62a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_quantum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_quantum\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfq\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_quantum'"
     ]
    }
   ],
   "source": [
    "import tensorflow_quantum as tfq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645e46bf-1913-474b-a651-89417adc756e",
   "metadata": {},
   "source": [
    "## What is quantum circuit optimization?\n",
    "\n",
    "Quantum circuits can often have Gates which when applied successively can result in identities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce7ea19-22e5-4e85-91ee-024fba99f1b0",
   "metadata": {},
   "source": [
    "## Why is it so hard?\n",
    "Circuit optimization is difficult due to the nature of the increasing complexity of large circuits, the ambiguity of the reward function, as well as the larger search space for the optimal circuit with the minimal gate count and circuit depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25071d84-9c62-479f-9a35-8ecd466aff8b",
   "metadata": {},
   "source": [
    "### What are some existing techniques??\n",
    "\n",
    "#### T-OPT\n",
    "\n",
    "#### ZX Calculus\n",
    "\n",
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08b9c8d0-3ee0-4aab-a1cf-b3a0922d514c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.qbraid/environments/tensorflow_uorhf3/pyenv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting seaborn\n",
      "  Downloading seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.3/293.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy!=1.24.0,>=1.17 in ./.qbraid/environments/tensorflow_uorhf3/pyenv/lib/python3.9/site-packages (from seaborn) (1.23.5)\n",
      "Requirement already satisfied: pandas>=0.25 in ./.qbraid/environments/tensorflow_uorhf3/pyenv/lib/python3.9/site-packages (from seaborn) (1.4.4)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in ./.qbraid/environments/tensorflow_uorhf3/pyenv/lib/python3.9/site-packages (from seaborn) (3.6.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.qbraid/environments/tensorflow_uorhf3/pyenv/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.qbraid/environments/tensorflow_uorhf3/pyenv/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.qbraid/environments/tensorflow_uorhf3/pyenv/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./.qbraid/environments/tensorflow_uorhf3/pyenv/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.qbraid/environments/tensorflow_uorhf3/pyenv/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./.qbraid/environments/tensorflow_uorhf3/pyenv/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in ./.qbraid/environments/tensorflow_uorhf3/pyenv/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.qbraid/environments/tensorflow_uorhf3/pyenv/lib/python3.9/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.qbraid/environments/tensorflow_uorhf3/pyenv/lib/python3.9/site-packages (from pandas>=0.25->seaborn) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.qbraid/environments/tensorflow_uorhf3/pyenv/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -rotobuf (/home/jovyan/.qbraid/environments/tensorflow_uorhf3/pyenv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: seaborn\n",
      "Successfully installed seaborn-0.12.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e4c921a-dc02-447d-b180-0ea31eacbf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf067eef-ac1b-484a-b63d-ad470390815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build random circuit dataset\n",
    "\n",
    "# Define the quantum circuit\n",
    "def create_quantum_circuit():\n",
    "    qubits = cirq.GridQubit.rect(2, 1)  # Example with 2 qubits\n",
    "    circuit = cirq.Circuit(\n",
    "        cirq.X(qubits[0]),\n",
    "        cirq.X(qubits[0]),\n",
    "        cirq.Y(qubits[0]),\n",
    "        cirq.CNOT(qubits[0], qubits[1]),\n",
    "        cirq.measure(qubits[0], key='m0'),\n",
    "        cirq.measure(qubits[1], key='m1')\n",
    "    )\n",
    "    return circuit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e9c1379-f1f9-415d-8937-9d5c3dd19ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define gate set to for assembly instruction / reward function\n",
    "\n",
    "# Define the reward function\n",
    "def compute_reward(result):\n",
    "    target_state = np.array([1 / np.sqrt(2), 0, 0, 1 / np.sqrt(2)])  # Example target states\n",
    "    probabilities = np.abs(result.state_vector()) ** 2\n",
    "    state = np.array([probabilities[0], probabilities[2], probabilities[1], probabilities[3]])\n",
    "    fidelity = np.abs(np.dot(target_state, state)) ** 2\n",
    "    return fidelity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a67709a2-fb04-45a2-a0ac-c3e601edb87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-29 19:09:47.693268: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-06-29 19:09:47.693320: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-06-29 19:09:47.693351: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jupyter-rickyyoung-40qbraid-2ecom): /proc/driver/nvidia/version does not exist\n",
      "2023-06-29 19:09:47.693679: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RLAgent' object has no attribute 'num_gates'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 88\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[1;32m     87\u001b[0m num_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m---> 88\u001b[0m training_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Print the training rewards\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode, reward \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(training_rewards):\n",
      "Cell \u001b[0;32mIn[5], line 63\u001b[0m, in \u001b[0;36mtrain_agent\u001b[0;34m(agent, environment, num_episodes)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[1;32m     62\u001b[0m     gates \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_gates\u001b[49m):\n\u001b[1;32m     64\u001b[0m         state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;28mlen\u001b[39m(agent\u001b[38;5;241m.\u001b[39mgate_set))[gates]\n\u001b[1;32m     65\u001b[0m         action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action(state)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RLAgent' object has no attribute 'num_gates'"
     ]
    }
   ],
   "source": [
    "## \n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cirq\n",
    "\n",
    "class RLAgent:\n",
    "    def __init__(self, gate_set):\n",
    "        self.gate_set = gate_set\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        num_actions = len(self.gate_set)\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(16, activation='relu', input_shape=(num_actions,)),\n",
    "            tf.keras.layers.Dense(16, activation='relu'),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        return model\n",
    "\n",
    "    def update(self, states, rewards):\n",
    "        self.model.fit(states, rewards, epochs=1, verbose=0)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        action_values = self.model.predict(np.array([state]))[0]\n",
    "        action_index = np.argmax(action_values)\n",
    "        action = self.gate_set[action_index]\n",
    "        return action\n",
    "\n",
    "def create_quantum_circuit(gate_set, num_gates):\n",
    "    qubits = cirq.LineQubit.range(num_gates)\n",
    "    circuit = cirq.Circuit()\n",
    "    for _ in range(num_gates):\n",
    "        gate = random.choice(gate_set)\n",
    "        target = random.choice(qubits)\n",
    "        circuit.append(gate(target))\n",
    "    return circuit\n",
    "\n",
    "def compute_reward(result):\n",
    "    # Define your reward computation logic here\n",
    "    pass\n",
    "\n",
    "class QuantumEnvironment:\n",
    "    def __init__(self, gate_set, num_gates):\n",
    "        self.gate_set = gate_set\n",
    "        self.num_gates = num_gates\n",
    "\n",
    "    def run(self, gates):\n",
    "        circuit = create_quantum_circuit(self.gate_set, self.num_gates)\n",
    "        circuit.append(gates)\n",
    "        simulator = cirq.Simulator()\n",
    "        result = simulator.run(circuit)\n",
    "        reward = compute_reward(result)\n",
    "        return reward\n",
    "\n",
    "# Main training loop\n",
    "def train_agent(agent, environment, num_episodes):\n",
    "    rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        gates = []\n",
    "        for _ in range(agent.num_gates):\n",
    "            state = np.eye(len(agent.gate_set))[gates]\n",
    "            action = agent.get_action(state)\n",
    "            gates.append(action)\n",
    "        reward = environment.run(gates)\n",
    "        agent.update(np.eye(len(agent.gate_set))[gates], np.array([reward]))\n",
    "        rewards.append(reward)\n",
    "    return rewards\n",
    "\n",
    "# Define the gate set for assembly instructions\n",
    "gate_set = [\n",
    "    cirq.X,\n",
    "    cirq.Y,\n",
    "    cirq.Z,\n",
    "    cirq.H,\n",
    "    cirq.CNOT,\n",
    "]\n",
    "\n",
    "# Create the agent and environment\n",
    "num_gates = 10\n",
    "agent = RLAgent(gate_set)\n",
    "environment = QuantumEnvironment(gate_set, num_gates)\n",
    "\n",
    "# Train the agent\n",
    "num_episodes = 100\n",
    "training_rewards = train_agent(agent, environment, num_episodes)\n",
    "\n",
    "# Print the training rewards\n",
    "for episode, reward in enumerate(training_rewards):\n",
    "    print(f\"Episode {episode+1}: Reward = {reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0db264c-9e11-4a16-9e34-41d6d3bbf1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The assembly instruction is the set of gates you are interested in using\n",
    "\n",
    "\n",
    "\n",
    "# Define the quantum environment\n",
    "class QuantumEnvironment:\n",
    "    def __init__(self):\n",
    "        self.circuit = create_quantum_circuit()\n",
    "\n",
    "    def run(self, params):\n",
    "        simulator = cirq.Simulator()\n",
    "        resolved_circuit = cirq.resolve_parameters(self.circuit, params)\n",
    "        result = simulator.simulate(resolved_circuit)\n",
    "        reward = compute_reward(result)\n",
    "        return reward\n",
    "\n",
    "# Define the reinforcement learning agent\n",
    "class RLAgent:\n",
    "    def __init__(self):\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(16, activation='relu', input_shape=(2,)),\n",
    "            tf.keras.layers.Dense(16, activation='relu'),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        return model\n",
    "\n",
    "    def update(self, states, rewards):\n",
    "        self.model.fit(states, rewards, epochs=1, verbose=0)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        action = self.model.predict(np.array([state]))\n",
    "        return action\n",
    "\n",
    "# Main training loop\n",
    "def train_agent(agent, num_episodes):\n",
    "    environment = QuantumEnvironment()\n",
    "    rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        state = np.random.uniform(low=-np.pi, high=np.pi, size=(2,))\n",
    "        action = agent.get_action(state)\n",
    "        reward = environment.run(action)\n",
    "        rewards.append(reward)\n",
    "        agent.update(np.array([state]), np.array([reward]))\n",
    "    return rewards\n",
    "\n",
    "# Create the agent and train it\n",
    "agent = RLAgent()\n",
    "num_episodes = 100\n",
    "training_rewards = train_agent(agent, num_episodes)\n",
    "\n",
    "# Print the training rewards\n",
    "for episode, reward in enumerate(training_rewards):\n",
    "    print(f\"Episode {episode+1}: Reward = {reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc19632c-f8a4-4035-8e3b-a2a6add2d6e6",
   "metadata": {},
   "source": [
    "In this code, we define a quantum circuit using Cirq and a reward function that computes the fidelity between the output state of the circuit and a target state. The QuantumEnvironment class represents the quantum environment in which the agent interacts. The RLAgent class represents the reinforcement learning agent, which uses a neural network model built with TensorFlow to approximate the optimal action given a state. The train_agent function is the main training loop, where the agent interacts with the environment and updates its model based on the observed rewards.\n",
    "\n",
    "Please note that this is just a basic starter code, and you may need to modify and extend it depending on your specific requirements and the complexity of the quantum circuits you want to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db43f04-c518-4828-9332-a303a5b0cabf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a408c34-a3aa-4da7-b58c-d2489635be1e",
   "metadata": {},
   "source": [
    "Ref:\n",
    "\n",
    "https://arxiv.org/pdf/2103.07585.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12e1b3c-a6ee-44a6-a6ec-b54560b2cf9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78783ee0-fb4f-4638-b990-e932077fde83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 [Default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
